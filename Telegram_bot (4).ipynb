{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7ZJ2G50bWWz",
        "outputId": "9820ae61-81d2-4ea2-c0f5-de0c0431feca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aiogram in /usr/local/lib/python3.11/dist-packages (3.21.0)\n",
            "Requirement already satisfied: aiofiles<24.2,>=23.2.1 in /usr/local/lib/python3.11/dist-packages (from aiogram) (24.1.0)\n",
            "Requirement already satisfied: aiohttp<3.13,>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from aiogram) (3.11.15)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from aiogram) (2025.6.15)\n",
            "Requirement already satisfied: magic-filter<1.1,>=1.0.12 in /usr/local/lib/python3.11/dist-packages (from aiogram) (1.0.12)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from aiogram) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions<=5.0,>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from aiogram) (4.14.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.13,>=3.9.0->aiogram) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.13,>=3.9.0->aiogram) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.13,>=3.9.0->aiogram) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.13,>=3.9.0->aiogram) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.13,>=3.9.0->aiogram) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.13,>=3.9.0->aiogram) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.13,>=3.9.0->aiogram) (1.20.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.4.1->aiogram) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.4.1->aiogram) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.4.1->aiogram) (0.4.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp<3.13,>=3.9.0->aiogram) (3.10)\n"
          ]
        }
      ],
      "source": [
        "pip install aiogram"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEcI2gDrc8va",
        "outputId": "6db2274b-8138-4423-ca89-ab3450acdf69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.93.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from aiogram import Bot, Dispatcher, types\n",
        "from aiogram.filters import Command\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "import openai"
      ],
      "metadata": {
        "id": "MRP7r0i4ddq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('Openai_api_key')\n",
        "os.environ['TELEGRAM_BOT_TOKEN'] = userdata.get('TELEGRAM_BOT_TOKEN')"
      ],
      "metadata": {
        "id": "gKHf1qaEeCfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dp = Dispatcher()"
      ],
      "metadata": {
        "id": "o-d2bBR0OpOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Gpt_context:\n",
        "  \"This will store previous responses from chatgpt\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self.response =\"\""
      ],
      "metadata": {
        "id": "7TIEhrgJQE5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = Gpt_context()"
      ],
      "metadata": {
        "id": "jfN2_TQCQKft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dp.message(Command('start'))\n",
        "async def command_start_handler(message: types.Message):\n",
        "    \"This handler will recieve the message with '/start' command\"\n",
        "    await message.answer(\"hii im a bot made bt rajvansh. \\n how can i assist you?\")"
      ],
      "metadata": {
        "id": "MwMfxKChQPrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dp.message(Command('help'))\n",
        "async def command_start_handler(message: types.Message):\n",
        "    \"This handler will recieve the message with '/help' command\"\n",
        "    help_command = \"\"\"\n",
        "    HELLO bro , Tension nai leneka apun hai na .. koi ek command type kar -\n",
        "    /start\n",
        "    /help\"\"\"\n",
        "    await message.reply(help_command)"
      ],
      "metadata": {
        "id": "Q7yBWKHOQT68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this at the top (after imports)\n",
        "from collections import defaultdict\n",
        "import tiktoken  # For token counting\n",
        "\n",
        "# Initialize conversation history\n",
        "conversations = defaultdict(list)\n",
        "MAX_HISTORY_TOKENS = 3000  # Adjust based on your token limits\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = openai.AsyncOpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
        "\n",
        "# Add reset command handler\n",
        "@dp.message(Command('reset'))\n",
        "async def reset_conversation(message: types.Message):\n",
        "    chat_id = message.chat.id\n",
        "    conversations[chat_id] = []  # Clear history\n",
        "    await message.answer(\"Conversation history cleared! Let's start fresh.\")\n",
        "\n",
        "# Updated message handler\n",
        "@dp.message()\n",
        "async def chatgpt(message: types.Message):\n",
        "    chat_id = message.chat.id\n",
        "\n",
        "    # Add user message to history\n",
        "    conversations[chat_id].append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": message.text\n",
        "    })\n",
        "\n",
        "    try:\n",
        "        # Get response from OpenAI with full history\n",
        "        response = await client.chat.completions.create(\n",
        "            model='gpt-4o-mini',\n",
        "            messages=conversations[chat_id],\n",
        "            temperature=0.7\n",
        "        )\n",
        "        response_content = response.choices[0].message.content\n",
        "\n",
        "        # Add assistant response to history\n",
        "        conversations[chat_id].append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": response_content\n",
        "        })\n",
        "\n",
        "        # Truncate history if too long\n",
        "        conversations[chat_id] = truncate_history(\n",
        "            conversations[chat_id],\n",
        "            MAX_HISTORY_TOKENS\n",
        "        )\n",
        "\n",
        "        await message.reply(response_content)\n",
        "\n",
        "    except Exception as e:\n",
        "        await message.reply(f\"Error: {str(e)}\")\n",
        "        conversations[chat_id].pop()  # Remove failed user message\n",
        "\n",
        "# Token counting and history truncation\n",
        "def truncate_history(history, max_tokens):\n",
        "    encoder = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "    total_tokens = 0\n",
        "    new_history = []\n",
        "\n",
        "    # Process messages in reverse (keep most recent)\n",
        "    for msg in reversed(history):\n",
        "        msg_tokens = len(encoder.encode(msg[\"content\"]))\n",
        "        if total_tokens + msg_tokens > max_tokens:\n",
        "            break\n",
        "        new_history.insert(0, msg)  # Add to front\n",
        "        total_tokens += msg_tokens\n",
        "\n",
        "    return new_history"
      ],
      "metadata": {
        "id": "G3XOOjVDVH_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@dp.message()\n",
        "#async def repeat(message: types.Message):\n",
        " # \"Handles all other messages apart from start and help command\"\n",
        "\n",
        " # await message.reply(message.text)\n",
        "\n",
        "@dp.message()\n",
        "async def chatgpt(message: types.Message):\n",
        "  \"This will process the messages and will get response from openai\"\n",
        "  response = openai.chat.completions.create(\n",
        "      model='gpt-4o-mini',\n",
        "      messages=[\n",
        "          {'role': 'user', 'content': message.text}\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  value = response.choices[0].message.content\n",
        "  print(\"Bot Response:\", value)\n",
        "  await message.reply(value)"
      ],
      "metadata": {
        "id": "Q5QhOb7CQXpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def main() -> None:\n",
        "  bot = Bot(os.environ['TELEGRAM_BOT_TOKEN'])\n",
        "  await dp.start_polling(bot)"
      ],
      "metadata": {
        "id": "wkZ4mqUFQukp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  logging.basicConfig(level=logging.INFO, stream=sys.stdout)\n",
        "  await main()"
      ],
      "metadata": {
        "id": "iRDqNflzQ0kL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Dn7ZPusQ3xY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}